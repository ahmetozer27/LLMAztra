# === train_tokenizer.py ===

from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors
from transformers import PreTrainedTokenizerFast
import os
import json
import logging
from config import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def jsonl_to_text():
    """JSONL dosyasÄ±nÄ± metin dosyasÄ±na Ã§evir"""
    input_path = config["dataset_path_jsonl"]
    output_path = config["dataset_path_txt"]

    if not os.path.exists(input_path):
        logger.error(f"JSONL dosyasÄ± bulunamadÄ±: {input_path}")
        return False

    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    total_lines = 0
    processed_lines = 0

    with open(input_path, "r", encoding="utf-8") as f_in, \
            open(output_path, "w", encoding="utf-8") as f_out:

        for i, line in enumerate(f_in, start=1):
            total_lines += 1
            line = line.strip()

            if not line:
                continue  # BoÅŸ satÄ±rÄ± atla

            try:
                data = json.loads(line)
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ HatalÄ± JSONL satÄ±rÄ± {i}: {e}")
                continue

            # FarklÄ± JSON formatlarÄ±nÄ± destekle
            text_parts = []

            # Soru-cevap formatÄ±
            if "question" in data and "answer" in data:
                question = data["question"].strip()
                answer = data["answer"].strip()
                if question and answer:
                    text_parts.append(f"Soru: {question}")
                    text_parts.append(f"Cevap: {answer}")

            # Sadece text alanÄ±
            elif "text" in data:
                text = data["text"].strip()
                if text:
                    text_parts.append(text)

            # Prompt-completion formatÄ±
            elif "prompt" in data and "completion" in data:
                prompt = data["prompt"].strip()
                completion = data["completion"].strip()
                if prompt and completion:
                    text_parts.append(f"{prompt} {completion}")

            # Input-output formatÄ±
            elif "input" in data and "output" in data:
                input_text = data["input"].strip()
                output_text = data["output"].strip()
                if input_text and output_text:
                    text_parts.append(f"{input_text} {output_text}")

            # DiÄŸer formatlar iÃ§in tÃ¼m string alanlarÄ± birleÅŸtir
            else:
                for key, value in data.items():
                    if isinstance(value, str) and value.strip():
                        text_parts.append(value.strip())

            if text_parts:
                # Ã–zel tokenlarla Ã§evir
                full_text = "<bos> " + " ".join(text_parts) + " <eos>"
                f_out.write(full_text + "\n")
                processed_lines += 1

            # Ä°lerleme raporu
            if i % 10000 == 0:
                logger.info(f"Ä°ÅŸlenen satÄ±r: {i}")

    logger.info(f"âœ… DÃ¶nÃ¼ÅŸtÃ¼rme tamamlandÄ±!")
    logger.info(f"   Toplam satÄ±r: {total_lines}")
    logger.info(f"   Ä°ÅŸlenen satÄ±r: {processed_lines}")
    logger.info(f"   Ã‡Ä±ktÄ± dosyasÄ±: {output_path}")

    return processed_lines > 0


def train_tokenizer():
    """Tokenizer eÄŸitimi"""
    logger.info("ğŸš€ Tokenizer eÄŸitimi baÅŸlÄ±yor...")

    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    os.makedirs(config["tokenizer_path"], exist_ok=True)

    # EÄŸitim dosyasÄ±nÄ± kontrol et
    if not os.path.exists(config["dataset_path_txt"]):
        logger.error(f"EÄŸitim dosyasÄ± bulunamadÄ±: {config['dataset_path_txt']}")
        return False

    # Dosya boyutunu kontrol et
    file_size = os.path.getsize(config["dataset_path_txt"])
    if file_size == 0:
        logger.error("EÄŸitim dosyasÄ± boÅŸ!")
        return False

    logger.info(f"EÄŸitim dosyasÄ± boyutu: {file_size / (1024 * 1024):.2f} MB")

    # Tokenizer oluÅŸtur
    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))

    # Pre-tokenizer (Byte-level BPE)
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)

    # Decoder
    tokenizer.decoder = decoders.ByteLevel()

    # Post-processor
    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)

    # Ã–zel tokenlar
    special_tokens = [
        "<pad>",  # Padding token
        "<bos>",  # Begin of sequence
        "<eos>",  # End of sequence
        "<unk>",  # Unknown token
        "<mask>",  # Mask token (future use)
        "<sep>",  # Separator token
        "<cls>",  # Classification token
    ]

    # Trainer oluÅŸtur
    trainer = trainers.BpeTrainer(
        vocab_size=config["vocab_size"],
        min_frequency=2,
        special_tokens=special_tokens,
        show_progress=True,
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()
    )

    logger.info(f"Tokenizer eÄŸitimi baÅŸlÄ±yor...")
    logger.info(f"  Kelime daÄŸarcÄ±ÄŸÄ± boyutu: {config['vocab_size']}")
    logger.info(f"  Ã–zel tokenlar: {len(special_tokens)}")

    # EÄŸitim
    try:
        tokenizer.train(files=[config["dataset_path_txt"]], trainer=trainer)
        logger.info("âœ… Tokenizer eÄŸitimi tamamlandÄ±!")
    except Exception as e:
        logger.error(f"âŒ Tokenizer eÄŸitimi baÅŸarÄ±sÄ±z: {e}")
        return False

    # Kaydet
    tokenizer_path = os.path.join(config["tokenizer_path"], "tokenizer.json")
    tokenizer.save(tokenizer_path)
    logger.info(f"ğŸ’¾ Tokenizer kaydedildi: {tokenizer_path}")

    # HuggingFace tokenizer olarak kaydet
    try:
        hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)

        # Ã–zel tokenlarÄ± ayarla
        hf_tokenizer.bos_token = "<bos>"
        hf_tokenizer.eos_token = "<eos>"
        hf_tokenizer.unk_token = "<unk>"
        hf_tokenizer.pad_token = "<pad>"
        hf_tokenizer.mask_token = "<mask>"

        # Kaydet
        hf_tokenizer.save_pretrained(config["tokenizer_path"])
        logger.info(f"ğŸ’¾ HuggingFace tokenizer kaydedildi: {config['tokenizer_path']}")

    except Exception as e:
        logger.warning(f"âš ï¸ HuggingFace tokenizer kaydedilemedi: {e}")

    # Test et
    test_tokenizer(tokenizer_path)

    return True


def test_tokenizer(tokenizer_path):
    """Tokenizer test fonksiyonu"""
    logger.info("ğŸ§ª Tokenizer test ediliyor...")

    try:
        # Tokenizer yÃ¼kle
        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)

        # Test metinleri
        test_texts = [
            "Merhaba, nasÄ±lsÄ±n?",
            "Bu bir test metnidir.",
            "TÃ¼rkÃ§e karakterler: ÄŸÃ¼ÅŸÄ±Ã¶Ã§",
            "123 sayÄ±lar ve !@# Ã¶zel karakterler",
            "Uzun bir metin Ã¶rneÄŸi: " + "kelime " * 20
        ]

        for text in test_texts:
            # Encode
            tokens = tokenizer.encode(text)

            # Decode
            decoded = tokenizer.decode(tokens)

            logger.info(f"ğŸ“ Orijinal: {text}")
            logger.info(f"ğŸ”¢ Tokenlar: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
            logger.info(f"ğŸ”¤ Token sayÄ±sÄ±: {len(tokens)}")
            logger.info(f"ğŸ”„ Decode: {decoded}")
            logger.info("-" * 40)

        # Ã–zel tokenlarÄ± test et
        special_tests = {
            "<bos>": tokenizer.bos_token_id if hasattr(tokenizer, 'bos_token_id') else None,
            "<eos>": tokenizer.eos_token_id if hasattr(tokenizer, 'eos_token_id') else None,
            "<pad>": tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') else None,
            "<unk>": tokenizer.unk_token_id if hasattr(tokenizer, 'unk_token_id') else None,
        }

        logger.info("ğŸ¯ Ã–zel tokenlar:")
        for token, token_id in special_tests.items():
            logger.info(f"  {token}: {token_id}")

        logger.info("âœ… Tokenizer test baÅŸarÄ±lÄ±!")

    except Exception as e:
        logger.error(f"âŒ Tokenizer test baÅŸarÄ±sÄ±z: {e}")


def main():
    """Ana fonksiyon"""
    logger.info("ğŸ”§ Tokenizer eÄŸitim sÃ¼reci baÅŸlÄ±yor...")

    # 1. JSONL'den text'e Ã§evir (eÄŸer JSONL varsa)
    if os.path.exists(config["dataset_path_jsonl"]):
        logger.info("ğŸ“„ JSONL dosyasÄ± bulundu, metne Ã§evriliyor...")
        if not jsonl_to_text():
            logger.error("âŒ JSONL dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±sÄ±z!")
            return

    # 2. Text dosyasÄ±nÄ± kontrol et
    if not os.path.exists(config["dataset_path_txt"]):
        logger.error(f"âŒ EÄŸitim dosyasÄ± bulunamadÄ±: {config['dataset_path_txt']}")
        logger.info("ğŸ’¡ LÃ¼tfen veri dosyanÄ±zÄ± hazÄ±rlayÄ±n veya JSONL dosyasÄ±nÄ± yerleÅŸtirin.")
        return

    # 3. Tokenizer eÄŸit
    if train_tokenizer():
        logger.info("ğŸ‰ Tokenizer eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!")
    else:
        logger.error("âŒ Tokenizer eÄŸitimi baÅŸarÄ±sÄ±z!")


